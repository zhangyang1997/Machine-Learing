**Stochastic Bandits**

+ **名词解释**
  + Stochastic：随机的
  + regret：遗憾
  + uniform exploration：均匀探索。

**1.Model and examples**

+ 符号说明1：
  + arm的数量为$K.$
  + arm的序号为$a.$
  + 选择的轮数为$T.$
  + 第$t$轮选择序号为$a_t$的arm的reward为$r_t\in [0,1].$
+ M-a bandits问题定义：每一轮选择一个arm，如何使累积的reward最大？
+ 基本假设
  + ①该问题属于bandit feedback。
  + ②每一次arm下的reward之间是IID的。
  + ③每轮的reward是有限的，为了简单将reward限制在[0,1]。
+ 符号说明2：
  + $K$个arm的reward组成向量$\mu \in [0,1]^{K}.$
  + 序号为$a$的arm的平均reward为$\mu(a) = E[D_a].$标量
  + reward分布表示为$D_a.$
+ 伯努利分布
  + 定义：事件成功概率为1，否则概率为0，又称为两点分布或者0-1分布。这是最简单的reward分布。
  + 期望：$E[X]=p.$ 当reward分布是伯努利分布时，序号为$a$的arm的平均reward就等于$p.$

+ 符号说明3：

  + arm序号为$a$，数量为$K$，arm的集合为$A.$
  + 轮序为$t$，轮数为$T$。
  + 序号为$a$的arm的期望reward为$\mu(a) = E[D_a]$
  + 序号为$a$的最大reward为$\mu^{*} = \max\limits_{a\in A}\mu(a).$
  + 最大reward和平均rewad的差值为$\Delta(a)=\mu^{*}-\mu(a)$，又称为序号$a$的arm的gap.
  + 最大reward对应的arm的序号为$a^{*}.$
  + 集合$\{1,2,...,n\}$表示为$[n].$

+ 给定多个问题实例，如何对比评价一个M-a bandits算法的好坏？

  + 比较算法的累积sum(reward)和最优arm基准$\mu^{*}\cdot T$，计算$T$轮后的regret。

  + 公式
    $$
    R(T)=\mu^{*} \cdot T-\sum_\limits{t=1}^{T} \mu\left(a_{t}\right)
    $$

  + 说明：在第$t$轮选择的arm时随机的，所以$R(T)$是一个随机变量。通常讨论期望$E[R(T)].$

+ $R(T)$和$E[R(T)]$的区别

  + $R(T)$表示已发生的累积regret，有时也称pseudo-reget.
  + $E[R(T)]$表示期望regret.

**2.Simple algorithms: uniform exploration**

+ **Explore-First算法**

  ```
  1.探索过程:每个arm选择N次。
  2.选择平均reward最大的arm，记作amax。
  3.利用过程:剩下的轮次都选择amax。
  ```

  + 符号说明

    + 探索过程每个action的平均reward为$\bar{\mu}(a).$
    + 序号为$a$的arm的期望reward为$\mu(a) = E[D_a]$.

  + 总体目标：$\min|\bar{\mu}(a)-\mu(a)|$

  + 使用Hoeffding不等式量化平均reward和期望reward的差值

    + 符号说明：序号为$a$的arm的置信半径为$r(a)$，探索过程选择每个arm的次数为$N$，选择的总轮数为$T$.

    + 公式
      $$
      \begin{align}
      & r(a)=\sqrt{\frac{2 \log T}{N}}\\
      & \operatorname{Pr}\{|\bar{\mu}(a)-\mu(a)| \leq r(a)\} \geq 1-\frac{2}{T^{4}}
      \end{align}
      $$

  + clean事件：满足Hoeffding不等式的事件，又称为高概率事件。

  + bad事件：不是clean事件的事件。

  + 如何计算regret的上界和regret期望的上界?

    >**符号说明：arm数量为$K$，最优arm为$a^{*}$。**
  >
    >**1.当$K=2$**
    >
    >**如果通过算法选择了不是最优arm的$a$，那么$\bar{\mu}(a)>\bar{\mu}\left(a^{*}\right)$成立。**
    >
    >**因为是clean事件，所以$\mu(a)+r(a) \geq \bar{\mu}(a)>\bar{\mu}\left(a^{*}\right) \geq \mu\left(a^{*}\right)-r\left(a^{*}\right)$成立。**
    >
    >**化简得到：$\mu\left(a^{*}\right)-\mu(a) \leq r(a)+r\left(a^{*}\right)=O(\sqrt{\frac{\log T}{N}})$.**
    >
    >**所以**
    >
    >+ **利用过程每轮增加regret的最多为$O(\sqrt{\frac{\log T}{N}})$.**
    >
    >+ **利用过程的轮数为$T-KN=T-2N$**.
    >
    >+ **利用过程累积的regret上限为$O(\sqrt{\frac{\log T}{N}} \times T)$.**
    >
    >+ **探索过程每轮增加的regret最多为$1$**
    >+ **探索过程累积的regret上限为$N$.**
    >
    >+ **探索过程+利用过程的regret的上界为**
    >
    >$$
    >\begin{aligned}
    >R(T) & \leq N+O(\sqrt{\frac{\log T}{N}} \times(T-2 N)) \\
    >& \leq N+O(\sqrt{\frac{\log T}{N}} \times T)
    >\end{aligned}
    >$$
    >
    >**因为$N$递增，$\sqrt{\frac{\log T}{N}} \times T$递减，为了让二者趋于相等，令$N=T^{2 / 3}(\log T)^{1 / 3}$，**
    >
    >**得到**
    >$$
    >R(T) \leq O\left(T^{2 / 3}(\log T)^{1 / 3}\right)
    >$$
    >**regret的期望为**
    >$$
    >\begin{aligned}
    >&E [R(T)]= E [R(T) | \text { clean event }] \times \operatorname{Pr}[\text { clean event }]\\
    >&\begin{aligned}
    >&+ E [R(T) | \text { bad event }] \times \operatorname{Pr}[\text { bad event }] \\
    >\leq & E [R(T) | \text { clean event }]+T \times O\left(T^{-4}\right) \\
    >\leq & O\left(\sqrt{\log T} \times T^{2 / 3}\right)
    >\end{aligned}
    >\end{aligned}
    >$$
    >**end proof.**
    >
    >**2.当K>2时**
    >
    >**探索过程每轮增加的regret最多为$K$，累积regret的上限为$KN$.**
    >
    >**与$K=2$的证明方式相似，令$N=(T / K)^{2 / 3} \cdot O(\log T)^{1 / 3}$，得到**
    >$$
    >E [R(T)] \leq T^{2 / 3} \times O(K \log T)^{1 / 3}
    >$$
    >**end proof 2.**
  
+ **$\epsilon$-贪心算法**

  ```
  for each round t = 1,2,...,T do
  	if rand([0,1]) < epsilon then
  		explore
  	else
  		exploit
  	end if
  end for
  ```

  + 说明：当$\epsilon_t$与$ \frac{1}{t^{1/3}}$成正比，探索需要的轮次大约为$t^{\frac{2}{3}}$，(当探索优先算法的$T=t$，两个算法的探索次数相同)，随着轮次的增加，探索概率下降。

  + 说明：令$\epsilon_{t}=t^{-1 / 3} \cdot(K \log t)^{1 /3}$，第$t$轮regret的上界为$E [R(t)] \leq t^{2 / 3}\cdot O(K \log t)^{1 /3}$。

**3.Advanced algorithms: adaptive exploration**

**4.Forward look: bandits with initial information**

**5.Bibliorgraphic remarks and further directions**

**6.Exercises and Hints**