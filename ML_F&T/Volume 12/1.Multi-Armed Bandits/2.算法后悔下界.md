**Lower Bounds**

**简介**(35-36)

+ 35页段1：这一章内容讲的是赌博机算法不能解决的问题。

  + **所有算法regret的下界是$\Omega(\sqrt{(KT)})$。**
  + **实例依赖算法的regret下界$\Omega(logT)。$**

+ 说明1：regret上界形式为$C\cdot f(T)$，$f(\cdot)$与$\mu$无关，$C$与$T$无关。如果$C$和$\mu$无关，称为实例独立；否成称为实例依赖。

+ 35页定理2.1：一个随机多臂赌博机，选择轮数为$T$，arm数量为$K$，对于任意一个赌博机算法，存在一个实例满足$E [R(T)] \geq \Omega(\sqrt{K T})$.

+ 35页说明：定理2.1的下界是最坏情况，因为可能有许多实例的regret比$\Omega(\sqrt{(KT)})$小；为了证明定理2.1，构造一个实例的集合$F$，实例能欺骗任何算法；证明定理2.1可以描述为：

  + ①证明任意算法对$F$中的某些实例都有很高的regret。
  + ②定义一个随机的实例是一个$F$上的分布。然后证明任意算法都对这个分布有很高的regret期望。

+ 36页Remark2.1：用②说明①，如果任意算法都对一个实例的分布有很高的regret期望，那么会存在一个高regret的实例。用①说明②，如果$|F|$是确定的常量，即任意算法对一些实例有很高的regret，那么期望regret最小为$\frac{H}{|F|}$，但是$|F|$很大就不成立。

  + ①更严格的说法是，对于$F$中的固定一部分的实例，regret很高；那么可以说明②是不管$|F|$多大，regret在所有实例上均匀分布。超过H的增加了。

+ 36页中间例子：0-1reward分布满足下面公式：
  $$
  I _{j}=\left\{\begin{array}{ll}
  \mu_{i}=(1+\epsilon) / 2 & \text { arm } i=j \\
  \mu_{i}=1 / 2 & \text { arm } i \neq j
  \end{array}\right.
  $$

  + 符号说明：
    + $i$和$j$是arm的序号。
    + $\mu_i$是arm$i$的reward。
    + 参数$\epsilon>0$
  + reward分布说明：有$K$个arm，【$I_j$】表示arm j是有偏期望reward的arm，其他的arm是无偏期望reward的arm的【实例情况】。
  + 上一章的公式1.7结论是每个arm采用$\Omega(1/\epsilon^{2})$次可以得到regret上界，此时一定高概率被区分。
  + 现在需要证明每个arm采用$\Omega(1/\epsilon^{2})$次才能判断这个arm的好坏，然后得出regret为$\Omega(K/\epsilon)$，通过代入$\epsilon=\Theta(\sqrt{(K/T)}$完成证明。这个证明非常巧妙。
  
+ 这一章和上一章的区别：

  + 上一章是每个arm被选择多少次才能高概率区分最优arm。这一章是要高概率区分最优arm至少需要每个arm选择多少次。
  + 上一章是求算法的regret上界，这一章是求算法的共同regret下界。

**1.Bacground on KL-divergence**(37-38)

+ KL散度：两种概率分布之间差异的期望。

  + 符号说明

    + 有限样本空间为$\Omega$
    + 在$\Omega$上有两个概率分布$p$和$q$

  + 公式

    + $$
      KL (p, q)=\sum_{x \in \Omega} p(x) \ln \frac{p(x)}{q(x)}= E _{p}\left[\ln \frac{p(x)}{q(x)}\right]
      $$

+ Remark2.2：KL散度的定义和性质可以推广到无限样本空间，但是我们只用到了有限样本空间的定义和性质。

+ Remark2.3：有一个有限样本空间$x_1,...,x_n\in \Omega$，满足一个未知分布$p^{*}$。假设这个分布是$p$或者是$q$，通过样本和分布的对数似然比判断$p^{*}$是$p$还是$q$。

  + 对数似然比：

    + $$
      \Lambda_{n}:=\sum_{i=1}^{n} \frac{\log p\left(x_{i}\right)}{\log q\left(x_{i}\right)}
      $$

    + 如果对数似然比越大，$p^{*}$更可能是$p$。

  + 假设真实分布为$p$，当$n->∞$时，KL散度是对数似比的期望，即：

    + $$
    \lim _{n \rightarrow \infty} \Lambda_{n}= E \left[\Lambda_{n}\right]= KL (p, q) \quad \text { if } p^{*}=p
      $$
  
  + 符号说明：
  
    + 0-1分布的偏置随机量为$RC_\epsilon=\epsilon/2$
  
+ 38页定理2.2：KL散度的性质

  + 吉布斯不等式：对于任意两个分布$p,q$，当且仅当$p = q$时，$KL (p,q)≥0$。
  + 乘积分布的链式法则： $\Omega=\Omega_{1} \times \Omega_{1} \times \cdots \times \Omega_{n} $， $p=p_{1} \times p_{2} \times \cdots \times p_{n}$ ， $q=$ $q_{1} \times q_{2} \times \cdots \times q_{n}$，$p_{j}, q_{j}$ 是$\Omega_{j}$的分布，那么$KL (p, q)=\sum_{j=1}^{n} KL \left(p_{j}, q_{j}\right)$。
  + 皮斯科克不等式：对于任意事件$A\subset \Omega$，满足$2(p(A)-q(A))^{2} \leq KL (p, q)$
  + 随机coin：$ KL \left( RC _{\epsilon}, RC _{0}\right) \leq 2 \epsilon^{2}, \text { 且 } KL \left( RC _{0}, RC _{\epsilon}\right) \leq \epsilon^{2} \text { 对任意 }
    \epsilon \in\left(0, \frac{1}{2}\right)$

+ 对于$n$样本有两个随机coin：第一个是有偏的随机coin满足分布$p_j=RC_\epsilon$，第二个是无偏的随机coin满足分布$q_j$，其中$j\in[n]$，假设存在一个事件$A \subset \Omega$，如何证明当$\epsilon$很小时，$p(A)和$$q(A)$很接近？

  + 根据KL散度性质得到：

  + $$
    \begin{aligned}
    2(p(A)-q(A))^{2} & \leq \operatorname{KL}(p, q) ----- 皮斯克不等式\\
    &=\sum_{j=1}^{n} KL \left(p_{j}, q_{j}\right) ----链式法则\\
    & \leq n \cdot KL \left( RC _{\epsilon}, RC _{0}\right) ---p,q分布的定义\\
    & \leq 2 n \epsilon^{2}----随机coin的性质
    \end{aligned}
    $$

  + $$
    所以得到|p(A)-q(A)| \leq \epsilon \sqrt{n} \\
    特别的当\epsilon<\frac{1}{2 \sqrt{n}}，|p(A)-q(A)|<\frac{1}{2}
    $$

+ 39页引理2.3：一个样本空间$\Omega=\{0,1\}^{n}$，两个在$\Omega$上的分布$p=RC_\epsilon^{n}$和$q=RC_0^{n}$，当$\epsilon>0$，对于任意$A\subset \Omega$，满足$|p(A)-q(A)| \leq \epsilon \sqrt{n}$.

+ 注意：KL散度不满足对称性，这个性质我们不去讨论。

**2.A simple example：flipping one coin**(39)

+ 考虑一个biased random coin：一个分布在$\{0,1\}$上，未知均值$\mu\in [0,1]$，假设$\mu\in\{\mu_1,\mu_2\}$，且$\mu_1>\mu_2$。如何确定轮数T，使得误差最小。

+ 定义一个决策规则和规则满足的条件：

  $$
  \begin{array}{l}
  Rule：\Omega=\{0,1\}^{T}->\{High,Low\}\\
  \operatorname{Pr}\left[\text { Rule (观测值) }=\text { High } | \mu=\mu_{1}\right] \geq 0.99 \\
  \operatorname{Pr}\left[\text { Rule (观测值) }=\operatorname{Low} | \mu=\mu_{2}\right] \geq 0.99
  \end{array}
  $$

  + 根据上一章可知$T\text{~}(\mu_1-\mu_2)^{-2}$，可以高概率区分最优arm。
  + 这一章是要高概率区分最优arm，需要每个arm被选择多少次？

+ 39页引理2.4：$\mu_{1}=\frac{1+\epsilon}{2}$ ， $\mu_{2}=\frac{1}{2} $，要满足上面决策规则和规则满足的条件，$T>\frac{1}{4\epsilon^{2}}$.

**3.Flipping several coins："besr-arm identification"**(40-42)

+ 最优臂问题：考虑一个有K个arm的赌博机问题，其中每个arm都是一个有偏的随机硬币，其均值未知。每个arm的reward独立于一个固定的但未知的伯努利分布来。经过T轮后，算法输出一个预测的最优arm $y_T$。我们先只考虑预测准确的概率率，而不是后悔界。

+ 符号说明：

  + 臂的集合为$[K]$
  + 臂 $a$的平均回报为$\mu(a)$，
  + 一个情形为元组$I=(\mu(a):a\in [K]).$

+ 足够好的最优臂选择算法应该对于任意问题情形都满足  
  $$
  \operatorname{Pr}\left[\text { 预测} y_{T} \text { 是最优臂 } | I \right] \geq 0.99
  $$
  且需要满足$T\geq\Omega(K/\epsilon^{2})$。
  
+ 引理2.5：考虑一个最优臂选择问题，其中 $T \leq \frac{c K}{\epsilon^{2}}$ ，$c>0 $且足够小，选取任意算法，至少存在 $\lceil K / 3\rceil$ 个臂 $a$ 满足 $I _{a}$ 公式 (36页中间例子)， 那么预测准确的概率
  $$
  \operatorname{Pr}\left[y_{T}=a | I _{a}\right]<\frac{3}{4}
  $$

+ 推论2.6：假设$T$是满足引理2.5的$T$，确定最优arm的任意算法，按照均匀分布随机选择一个arm并且执行算法在实例 $I _{a} $ ，那么预测错误的概率有 $\operatorname{Pr}\left[y_{T} \neq a\right] \geq \frac{1}{12}$。

  + 注意选择满足$I_a$公式的臂的概率$P(A)=\frac{1}{3}$，满足$I_a$公式条件下预测错误的概率为$P(B|A)=\frac{1}{4}$，那么满足$I_a$公式且预测错误的概率为$P(A)P(B|A)=\frac{1}{3}\times \frac{1}{4}=\frac{1}{12}$

+ 定理2.7：确定选择轮数$T$和arm数量$K$，以及一个赌博机算法。随机均匀地选择一个臂$a$，并在问题实例$I_a$上执行该算法。那么赌博机算法满足：
  $$
  E [R(T)] \geq \Omega(\sqrt{K T})
  $$

**4.Proof of Lemma 2.5 for K24 arms**(43-45)

**5.Instance-dependent lower bounds**(45-47)——实例独立的后悔上界

定理2.8：对于所有实例$I$，没有算法能达到 $E [R(t)]=o\left(c_{ I } \log t\right)$ ， 其中常数 $c_{ I }$依赖于问题实例而不依赖次数 $t$.

定理2.9：

+ (1)确定arm的数量$K$，当$\alpha>0$，对于任意实例$I$满足$E [R(t)] \leq O\left(C_{ I , \alpha} t^{\alpha}\right)$ ，其中常数 $C_{ I , \alpha}$ 可以依赖实例 $I$ 和 $\alpha,$但是不依赖于次数 $t$ 。

+ (2)固定一个问题实例$I$，确定一个轮次$t_0$，当 $t \geq t_{0} \quad E [R(t)] \geq C_{ I } \ln (t)$，其中常数 $C_{ I }$ 依赖于问题实例$I$, 但是不依赖于次数 $t$。

定理2.10：对于满足定理2.9(1)的算法和实例$I$，

+ (a)$C_{ I }=\sum_{a: \Delta(a)>0} \frac{\mu^{*}\left(1-\mu^{*}\right)}{\Delta(a)}$
+ (b)对于$\epsilon>0$，$C_{ I }=\sum_{a: \Delta(a)>0} \frac{\Delta(a)}{\operatorname{KL}\left(\mu(a), \mu^{*}\right)}-\epsilon$

说明1：对于(a)，如果$\mu^{*}$不是0或者1，那么上界是下界的常数倍

说明2：对于(b)，下界(b)比下界(a)更严格。

