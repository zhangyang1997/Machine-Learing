**利普希茨赌博机**

利普希茨赌博机问题的臂之间存在相似信息，

在赌博机问题中，因为臂之间存在相似性的信息，所以相似的手臂有相似的期望奖励。例如，臂对应文档，相似度可以表示为文档的特征向量之间的距离。另一个例子是动态定价问题，臂对应价格，相似的价格通常对应相似的预期奖励。抽象地说，假设臂映射到一个已知的度量空间中的点，那么臂的期望奖励服从与这个度量空间相关的利普希茨条件。

首先考虑随机赌博机：每个臂$x$的奖励独立同分布，奖励期望$\mu(x)$。用$x$和$y$表示两个臂，对应于直线或度量空间中的点。假设奖励属于[0,1]。

**1.连续臂赌博机**

首先介绍连续臂赌博机(CAB)。臂的集合是$X =[0,1]$，期望奖励$\mu(·)$满足Lipschitz条件：
$$
|\mu(x)-\mu(y)| \leq L \cdot|x-y| \quad \text { 对任意两个臂 } x, y \in X
$$
其中$L$是一个已知的常数。连续臂赌博机的一个实例由平均奖励$\mu(·)$、轮次$T$和利普希茨常数$L$决定。注意连续臂赌博机有无限多的手臂，使用利普希茨条件更容易处理连续臂赌博机。

**1.1简单方法：固定离散化**

固定离散化，选择了一个固定的，有限的臂集$S\sub X$，叫做$X$的离散化，然后用$S$作为全集$X$的近似值。只关注$S$中的臂，使用多臂赌博机算法例如UCB1或连续淘汰算法。在$S$中增加更多的点使它更接近$X$，但也增加了算法的后悔，所以需要选择合适的$S$。

$S$中的最优臂表示为$\mu^{*}(S) = sup_ {x \in S} \mu(x)$。在每一轮中，算法只能期望接近期望的奖励$\mu^{*}(S)$，存在离散化误差：
$$
DE (S)=\mu^{*}(X)-\mu^{*}(S)
$$
我们可以将整个算法的期望后悔表示为一个和
$$
\begin{aligned}
E [R(T)] &=T \cdot \mu^{*}(X)-W( ALG ) \\
&=\left(T \cdot \mu^{*}(S)-W( ALG )\right)+T \cdot\left(\mu^{*}(X)-\mu^{*}(S)\right) \\
&=R_{S}(T)+T \cdot DE (S)
\end{aligned}
$$
其中$W (ALG)$为算法的总奖励，$R_ S (T)$为相对于$\mu^{*}(S)$的后悔。如果算法ALG在轮次为$T$和臂数为$K$的任何问题实例上都达到了最优后悔$O (\sqrt{KT logT})$，则算法的期望后悔上界为
$$
E [R(T)] \leq O(\sqrt{|S| T \log T})+\operatorname{DE}(S) \cdot T
$$
一个方法是使用$k$只臂的均匀离散化：将区间$[0,1]$划分为固定长度$\epsilon=\frac{1}{k-1}$的区间，其中$S$包含$\epsilon$整数倍个臂。显然$DE(S)\leq L\epsilon$。确实，如果$x^{*}$是$X$中的最优臂，$y$是$S$中离$x^{*}$最近的臂，那么$|x^{*} -y| \leq \epsilon$，$\mu\left(x^{*}\right)-\mu(y) \leq L \epsilon$。令$\epsilon=\left(T L^{2} / \log T\right)^{-1 / 3}$，得到定理4.1。

定理4.1：固定均匀离散化的连续臂赌博机的期望后悔上界为
$$
E [R(T)] \leq O\left(L^{1 / 3} \cdot T^{2 / 3} \cdot \log ^{1 / 3}(T)\right)
$$
