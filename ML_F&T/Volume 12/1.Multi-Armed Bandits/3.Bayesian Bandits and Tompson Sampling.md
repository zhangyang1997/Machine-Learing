**贝叶斯赌博机和汤普森抽样**

​		贝叶斯赌博机对应的算法成为汤普森抽样(后验抽样)。

​		贝叶斯赌博机问题：给定的臂数量$K$和轮数$T$，在第1章随机赌博机的基础上添加贝叶斯假设，问题实例$I$来自于某个已知的分布$P$。固定$K$，$T$，问题实例$I$对应的平均奖励向量$\mu∈[0,1]^{K}$，奖励分布为$D_a:a\in[K]$。分布$P$称为先验分布或者贝叶斯先验。目标是优化贝叶斯后悔$BR(T)$，和公式(1.1)的定义类似。
$$
R(T)=\mu^{*} \cdot T-\sum_{t=1}^{T} \mu\left(a_{t}\right)\\
\operatorname{BR}(T):=\underset{ I \sim P }{ E }[ E [R(T) | I ]]=\underset{ I \sim P }{ E }\left[\mu^{*} \cdot T-\sum_{t \in[T]} \mu\left(a_{t}\right)\right]
$$

+ 符号说明
  + 轮数为$T$
  + $T$轮后的后悔为$R(T)$
  + 臂数为$K$
  + 最优臂的回报为$\mu^{*}$
  + 第$t$轮选择臂$a$的回报为$\mu(a_t)$
  + 已知问题实例$I$发生的条件下，$R(T)$的期望为

​		贝叶斯赌博机服从贝叶斯统计方法：假设未知量是从一个已知的分布中抽样，然后根据期望对该分布进行优化。所有的问题实例有相同的最坏情况下的后悔上界$E [R (T)]$，即贝叶斯后悔上界是相同的。

​		为了简化问题，给出几个假设。

​		1.假设奖励分布来自于一个单参数的分布族。当奖励分布是伯努利分布和单位方差高斯。每只手臂$a$的奖励来自分布$D_\mu(a)$，其中$\mu(a)\in[0,1]$为平均奖励。保持单参数分布族不变。问题实例$I$完全由均值奖励向量$\mu \in[0,1]^{K}$决定，先验$P$是$[0,1]^{K}$的简单分布。

​		2.假设动作之后的奖励只能取有限个不同的值，且先验$P$有有限的支持，记作$F$。然后我们可以关注汤普森抽样的基本概念和论证，不必关注积分和概率密度的复杂性。同时，下面的定义和引理适用于任意的先验和奖励分布。

​		3.最优臂$a$对于支持$P$的每个平均奖励向量$\mu$都是唯一的。

**3.1贝叶斯赌博机的更新**

​		贝叶斯统计的一个基本操作是贝叶斯更新：更新给定新数据的先验分布。

​		**3.1.1术语和符号**

​		算法在第$t$轮后采集的数据为动作-奖励对序列
$$
H_{t}=\left(\left(a_{1}, r_{1}\right), \dots,\left(a_{t}, r_{t}\right)\right) \in( A \times R )^{t}
$$
称为$t$-历史。$H_t$是一个随机变量，取决于平均奖励向量$\mu$、算法和奖励分布。

​		给定一个固定的序列
$$
H=\left(\left(a_{1}^{\prime}, r_{1}^{\prime}\right), \ldots,\left(a_{t}^{\prime}, r_{t}^{\prime}\right)\right) \in( A \times R )^{t}
$$
​		对于某些赌博机算法，如果满足$Pr [H_t = H] > 0$，那么$H$称为可行$t$-历史。对应的赌博机算法沉稳给为$H$-一致算法。

​		例如$H$-诱导算法，它在每轮$s \in[t]$中都确定地选择臂 $a^{'}_{s}$。设$H_t$为所有可行$t$-历史的集合。因为每个奖励只能取有限多的值，所以$H_t$是有限的。特别地，对于伯努利奖励分布和对于所有的臂$a$使得$Pr[\mu(a)\in(0,1)] = 1$的先验分布，$H_t = (A \times\{0,1\})^{t}$ 。然后，确定一个可行的$t$-历史$H$，那么满足$H_t=H$条件下奖励概率分布为
$$
P _{H}( M ):=\operatorname{Pr}\left[\mu \in M | H_{t}=H\right], \quad \forall M \subset[0,1]^{K}
$$
​		对于任何$H$一致的赌博机算法，$P_H$是在$[0,1]^{K}$的奖励分布。

​		备注3.1：将经过$t$轮后的$P_H$称为贝叶斯后验概率。推导$P_H$的过程称为给定$H$条件下$P$的贝叶斯更新。

​		**3.1.2后验不依赖于算法**

​		关于贝叶斯强盗的一个基本事实是，分布P H不依赖于H一致的强盗算法收集的历史。因此，w.l.o.g.是H-induced算法。

​		引理3.1。对于所有H一致的强盗算法，其分布是相同的。

​		这个证明需要仔细的论证;特别重要的是，算法的行动概率是由历史决定的，而奖励分配是由所选择的行动决定的。

​		证明。对于任何给定的向量[0,1]K，证明单例集合M ={}的引理就足够了。因此，我们感兴趣的是{=}的条件概率。回想一下，有平均报酬(a)的报酬分布将概率D (a) (r)放在每个给定值r r上。

​		我们对t使用归纳法。基本情况是t = 0。为了更好地定义它，让我们将0历史定义为H 0 =，这样H =就是唯一可行的0历史。那么，所有的算法都是-一致的，条件概率Pr [= | H 0 = H]就是先验概率P()。

​		主要的论点是归纳步骤。考虑第一轮t1。把H写成一些可行的(t1)-历史h0和一个行动-奖励对(a,r)的连接。修正H一致的强盗算法，令
$$
\pi(a)=\operatorname{Pr}\left[a_{t}=a | H_{t-1}=H^{\prime}\right]
$$
表示在给定历史H为0的情况下，该算法在第t轮分配给每个手臂a的概率。注意，这个概率不依赖于平均回报向量。
$$
\begin{aligned}
\frac{\operatorname{Pr}\left[\mu=\tilde{\mu} \text { and } H_{t}=H\right]}{\operatorname{Pr}\left[H_{t-1}=H^{\prime}\right]}=& \operatorname{Pr}\left[\mu=\tilde{\mu} \text { and }\left(a_{t}, r_{t}\right)=(a, r) | H_{t-1}=H^{\prime}\right] \\
=& P _{H^{\prime}}(\tilde{\mu}) \\
& \operatorname{Pr}\left[\left(a_{t}, r_{t}\right)=(a, r) | \mu=\tilde{\mu} \text { and } H_{t-1}=H^{\prime}\right] \\
=& P _{H^{\prime}}(\tilde{\mu}) \\
& \operatorname{Pr}\left[r_{t}=r | a_{t}=a \text { and } \mu=\tilde{\mu} \text { and } H_{t-1}=H^{\prime}\right] \\
& \operatorname{Pr}\left[a_{t}=a | \mu=\tilde{\mu} \text { and } H_{t-1}=H^{\prime}\right] \\
=& P _{H^{\prime}}(\tilde{\mu}) \cdot D _{\tilde{\mu}(a)}(r) \cdot \pi(a)
\end{aligned}
$$
所以
$$
\operatorname{Pr}\left[H_{t}=H\right]=\pi(a) \cdot \operatorname{Pr}\left[H_{t-1}=H^{\prime}\right] \sum_{\tilde{\mu} \in F } P _{H^{\prime}}(\tilde{\mu}) \cdot D _{\tilde{\mu}(a)}(r)
$$
所以
$$
P _{H}(\tilde{\mu})=\frac{\operatorname{Pr}\left[\mu=\tilde{\mu} \text { and } H_{t}=H\right]}{\operatorname{Pr}\left[H_{t}=H\right]}=\frac{ P _{H^{\prime}}(\tilde{\mu}) \cdot D _{\tilde{\mu}(a)}(r)}{\sum_{\tilde{\mu} \in F } P _{H^{\prime}}(\tilde{\mu}) \cdot D _{\tilde{\mu}(a)}(r)}
$$
根据归纳假设，后验分布不依赖于算法。因此，上面的表达式也不依赖于算法。

​		因此，如果这些轮被置换，那么P H保持不变。

​		推论3.2。每当H 0 = ??0σ0σ(t)、r (t) ?: t [t] ?对于一些排列σ[t]。

​		3.2的话。引理3.1不应被视为理所当然。事实上，贝叶斯更新有两个非常自然的扩展，而这个引理并不适用于这两个扩展。首先，假设我们的条件是一个任意可观测的事件。也就是说，确定一组可行t -历史的H。对于任何具有Pr [H t H] > 0的算法，考虑事件{H t H]的后验分布
$$
\operatorname{Pr}\left[\mu \in M | H_{t} \in H \right], \quad \forall M \subset[0,1]^{K}
$$
​		这种分布可能依赖于强盗算法。举一个简单的例子，考虑一个有伯努利奖励的问题实例，三个手臂a = {a，一个0，一个00}，和一个单轮。假设H包含两个可行的1-历史，H = (a, 1)和H 0 = (a, 1)。两个算法，ALG和ALG 0，分别确定地选择臂a和臂a 0。那么分布(3.4)就等于ALG下的P H, ALG下的P H 0。

​		第二，假设我们的条件是一个轮的子集。s -history算法是一个有序元组，它针对的是s [T]轮的子集s [T]
$$
H_{S}=\left(\left(a_{t}, r_{t}\right): t \in S\right) \in( A \times R )^{|S|}
$$
对于任何可行的|S| -history H，给定事件{H S = H}，记为P H,S的后验分布为
$$
P _{H, S}( M ):=\operatorname{Pr}\left[\mu \in M | H_{S}=H\right], \quad \forall M \subset[0,1]^{K}
$$
然而，这种分布也可能依赖于强盗算法。考虑一个有伯努利奖励的问题实例，两个手臂a = {a，一个0}，两个回合。令S ={2}(即,我们只条件发生在第二轮)和H =(, 1)。考虑两个算法,ALG ALG 0,在第一轮选择不同的武器(比如ALG和ALG 0 0),并选择手臂在第二轮当且仅当他们收到的奖励1在第一轮。(3.6) ALG条件下h1 = (a, 1)， ALG条件下h1 = (a, 1)的分布(3.6)。