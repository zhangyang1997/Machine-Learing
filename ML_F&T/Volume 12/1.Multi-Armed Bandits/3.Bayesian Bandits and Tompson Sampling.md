**贝叶斯赌博机和汤普森抽样**

​		贝叶斯赌博机对应的算法成为汤普森抽样(后验抽样)。

​		贝叶斯赌博机问题：给定的臂数量$K$和轮数$T$，在第1章随机赌博机的基础上添加贝叶斯假设，问题实例$I$来自于某个已知的分布$P$。固定$K$，$T$，问题实例$I$对应的平均奖励向量$\mu∈[0,1]^{K}$，奖励分布为$D_a:a\in[K]$。分布$P$称为先验分布或者贝叶斯先验。目标是优化贝叶斯后悔$BR(T)$，和公式(1.1)的定义类似。
$$
R(T)=\mu^{*} \cdot T-\sum_{t=1}^{T} \mu\left(a_{t}\right)\\
\operatorname{BR}(T):=\underset{ I \sim P }{ E }[ E [R(T) | I ]]=\underset{ I \sim P }{ E }\left[\mu^{*} \cdot T-\sum_{t \in[T]} \mu\left(a_{t}\right)\right]
$$

+ 符号说明
  + 轮数为$T$
  + $T$轮后的后悔为$R(T)$
  + 臂数为$K$
  + 最优臂的回报为$\mu^{*}$
  + 第$t$轮选择臂$a$的回报为$\mu(a_t)$
  + 已知问题实例$I$来自服从分布$P$，$R(T)$的期望为$BR(T)$
  + 问题实例$I$对应的平均奖励向量为$\mu$

​		贝叶斯赌博机服从贝叶斯统计方法：假设未知量是从一个已知的分布中抽样，然后根据期望对该分布进行优化。所有的问题实例有相同的最坏情况下的后悔上界$E [R (T)]$，即贝叶斯后悔上界是相同的。

​		为了简化问题，给出几个假设。

​		1.假设奖励分布来自于一个单参数的分布族。例如奖励分布是伯努利分布和单位方差高斯分布。每只臂$a$的奖励服从分布$D_\mu(a)$，其中$\mu(a)\in[0,1]$为臂$a$的平均奖励。问题实例$I$完全由平均奖励向量$\mu \in[0,1]^{K}$决定，先验$P$是$[0,1]^{K}$的简单分布。

​		2.假设动作之后的奖励只能取有限个不同的值，且先验$P$有有限的支持，记作$F$。然后我们可以关注汤普森抽样的基本概念和论证，不必关注积分和概率密度的复杂性。同时，下面的定义和引理适用于任意的先验和奖励分布。

​		3.最优臂$a$对于支持$P$的每个平均奖励向量$\mu$都是唯一的。

**3.1贝叶斯赌博机的更新**

​		贝叶斯统计的一个基本操作是贝叶斯更新：更新给定新数据的先验分布。

**3.1.1术语和符号**

​		算法在第$t$轮后采集的数据为动作-奖励对序列
$$
H_{t}=\left(\left(a_{1}, r_{1}\right), \dots,\left(a_{t}, r_{t}\right)\right) \in( A \times R )^{t}
$$
称为$t$-历史。$H_t$是一个随机变量，取决于平均奖励向量$\mu$、算法和奖励分布。

​		给定一个固定的序列
$$
H=\left(\left(a_{1}^{\prime}, r_{1}^{\prime}\right), \ldots,\left(a_{t}^{\prime}, r_{t}^{\prime}\right)\right) \in( A \times R )^{t}
$$
​		如果满足$Pr [H_t = H] > 0$，那么$H$称为可行$t$-历史，对应的赌博机算法称为$H$-一致算法。

​		$H$-诱导算法，它在每轮$s \in[t]$中都确定地选择臂 $a^{'}_{s}$。

​		设$H_t$为所有可行$t$-历史的集合。

​		因为每个奖励只能取有限多的值，所以$H_t$是有限的。

​		特别地，对于伯努利奖励分布和对于所有的臂$a$使得$Pr[\mu(a)\in(0,1)] = 1$的先验分布$P$，$H_t = (A \times\{0,1\})^{t}$ 。

​		然后，确定一个可行$t$-历史$H$，那么满足$H_t=H$条件下奖励概率为
$$
P _{H}( M ):=\operatorname{Pr_{\mu \sim P}}\left[\mu \in M | H_{t}=H\right], \quad \forall M \subset[0,1]^{K}
$$
​		对于任何$H$一致的赌博机算法，$P_H$是在$[0,1]^{K}$的奖励分布。

​		备注3.1：将经过$t$轮后的$P_H$称为贝叶斯后验概率。推导$P_H$的过程称为给定$H$条件下$P$的贝叶斯更新。

**3.1.2后验不依赖于算法**

​		贝叶斯赌博机的$P_H$既不依赖于$H$一致算法，也不依赖于$H$诱导算法。

​		定理3.1：所有$H$一致算法的分布$P_H$是相同的。

​		注意：算法的动作概率是由历史决定，奖励分布是由动作决定，奖励分布与历史无关。

​		推论3.2：重排列$H$得到$H'$，$P_H=P_H'$。

**3.1.3后验作为新的先验**

+ 引理3.3：如果$H'$是可行$t'$历史，那么$P _{H \oplus H^{\prime}}=\left( P _{H}\right)_{H^{\prime}}$。即
  $$
  P _{H \oplus H^{\prime}}( M )=\operatorname{Pr}_{\mu \sim P _{H}}\left[\mu \in M | H_{t^{\prime}}=H^{\prime}\right], \quad \forall M \subset[0,1]^{K}
  $$

+ 可行$t$历史：满足$Pr [H_t = H] > 0$的$H$

+ $t$历史：$1-t$轮采集的动作奖励序列为$H_t$

+ 可行$t$历史$H$和可行$t'$历史$H'$串联连接为$H\oplus H'$

+ 在$H_t=H$条件下的奖励概率分布为$P_H$

**3.1.4独立先验**

+ 引理3.4：如果先验$P$独立，那么$P _{H}\left(\cap_{a \in A } M _{a}\right)=\prod_{a \in A } P _{H}^{a}\left( M _{a}\right)$.

**3.2算法实现**

**1.汤普森采样**

```
for t=1 to T:
	根据p[t,a|H]=p(a*|H)，选择臂a[t];
end
```

**2.汤普森采样(另一种表示)**

```
for t=1 to T:
	后验概率p[H]组成平均奖励向量u[t];
	根据u[t],选择最优臂a[t];
end
```

引理3.5：在$H_t$条件下，对于每一轮$t$，臂$a_t$，$\tilde{a}_t$是独立同分布。

**3.先验独立的汤普森采样**

```
for t=1 toT:
	后验概率p[H][a]组成每个臂a的平均奖励u[t][a]
	根据u[t][a]选择最优臂a;
end
```

**3.2.1计算**

虽然汤普森抽样在数学上定义良好，但在计算上可能效率不高。后验概率$P_H$为
$$
\begin{aligned}
P _{H}(\tilde{\mu}) &=\frac{\operatorname{Pr}\left[\mu=\tilde{\mu} \text { and } H_{t}=H\right]}{ P \left(H_{t}=H\right)} \\
&=\frac{ P (\tilde{\mu}) \cdot \operatorname{Pr}\left[H_{t}=H | \mu=\tilde{\mu}\right]}{\sum_{\tilde{\mu} \in F } P (\tilde{\mu}) \cdot \operatorname{Pr}\left[H_{t}=H | \mu=\tilde{\mu}\right]}, \quad \forall \tilde{\mu} \in F
\end{aligned}
$$
+ 计算条件概率$P[H_t=H|\mu=\tilde\mu]$的时间复杂度为$O(t)$；

+ 计算$P(H_t=H)$的时间复杂度为$O(t\cdot|F|)$；

+ 计算后验概率$P_H(\tilde\mu)$第$t$轮的时间复杂度至少为$O(t\cdot|F|)$。

更快的算法是连续贝叶斯更新：第$t$轮的后验概率$P_H$作为新的先验。新的选择为$(a_t,r_t)=(a,r)$，那么新的先验为$P_{H\oplus (a,r)}$。算法的原理是引理3.3。此时每轮计算后验概率的时间复杂度为$O(|F|)$。

假设有独立先验，可以分别对每个臂$a$进行贝叶斯更新，每轮只更新一个臂$a$的后验，后验概率$P_{H'}^{a}$为:
$$
\begin{aligned}
P _{H^{\prime}}^{a}(x) &=\operatorname{Pr}_{\mu(a) \sim P _{H}^{a}}\left[\mu(a)=x |\left(a_{t}, r_{t}\right)=(a, r)\right] \\
&=\frac{ P _{H}^{a}(x) \cdot D _{x}(r)}{\sum_{x \in F _{a}} P _{H}^{a}(x) \cdot D _{x}(r)}, \quad \forall x \in F _{a}
\end{aligned}
$$
时间复杂度为$O(|F_a|)$，不使用连续贝叶斯更新的$|F|=\prod_\limits{i=1}^{K}|F_{a_i}|$，所以时间复杂度指数倍下降。

简化问题：对于β-伯努利和高斯分布，计算后验概率的速度更快。假设只有一只臂$a$，$P$是平均奖励$\mu(a)$的先验，$H$是可行$t$历史，$REW_H$表示$H$历史的总奖励。

**β-伯努利**

已知后验概率$P_H$由先验$P$，轮次$t$，$H$历史的总奖励$REW_H$决定，假设先验$P$服从$[0,1]$均匀分布$U$，那么后验概率$U_H$服从$\beta$分布，其中$\alpha=1+REW_h$，$\beta=1+t$，记作$Beta(\alpha,\beta)$。为了保持一致，$Beta(1,1)=U$：如果$t=1$，后验概率$U_H$的历史为空，即$U_H=U$。

**高斯**

高斯共轭对是高斯奖励分布和高斯先验分布$P$的结合，$\mu$，$\mu_0$分别表示均值，$\sigma,\sigma_0$分别表示标准差，后验概率$P_H$也服从高斯分布，其中参数由$\mu,\mu_0,\sigma,\sigma_0$决定。

先验分布是共轭分布，后验分布也是共轭分布。

**3.3贝叶斯后悔分析**

定理3.6：汤普森抽样的贝叶斯后悔为$BR(T)=\bar{O}(\sqrt{K T} \log (T))$。

