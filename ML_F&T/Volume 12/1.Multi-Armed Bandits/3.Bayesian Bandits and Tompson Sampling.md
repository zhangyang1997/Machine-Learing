**贝叶斯赌博机和汤普森抽样**

​		贝叶斯赌博机对应的算法成为汤普森抽样(后验抽样)。

​		贝叶斯赌博机问题：给定的臂数量$K$和轮数$T$，在第1章随机赌博机的基础上添加贝叶斯假设，问题实例$I$来自于某个已知的分布$P$。固定$K$，$T$，问题实例$I$对应的平均奖励向量$\mu∈[0,1]^{K}$，奖励分布为$D_a:a\in[K]$。分布$P$称为先验分布或者贝叶斯先验。目标是优化贝叶斯后悔$BR(T)$，和公式(1.1)的定义类似。
$$
R(T)=\mu^{*} \cdot T-\sum_{t=1}^{T} \mu\left(a_{t}\right)\\
\operatorname{BR}(T):=\underset{ I \sim P }{ E }[ E [R(T) | I ]]=\underset{ I \sim P }{ E }\left[\mu^{*} \cdot T-\sum_{t \in[T]} \mu\left(a_{t}\right)\right]
$$

+ 符号说明
  + 轮数为$T$
  + $T$轮后的后悔为$R(T)$
  + 臂数为$K$
  + 最优臂的回报为$\mu^{*}$
  + 第$t$轮选择臂$a$的回报为$\mu(a_t)$
  + 已知问题实例$I$来自服从分布$P$，$R(T)$的期望为$BR(T)$
  + 问题实例$I$对应的平均奖励向量为$\mu$

​		贝叶斯赌博机服从贝叶斯统计方法：假设未知量是从一个已知的分布中抽样，然后根据期望对该分布进行优化。所有的问题实例有相同的最坏情况下的后悔上界$E [R (T)]$，即贝叶斯后悔上界是相同的。

​		为了简化问题，给出几个假设。

​		1.假设奖励分布来自于一个单参数的分布族。例如奖励分布是伯努利分布和单位方差高斯分布。每只臂$a$的奖励服从分布$D_\mu(a)$，其中$\mu(a)\in[0,1]$为臂$a$的平均奖励。问题实例$I$完全由平均奖励向量$\mu \in[0,1]^{K}$决定，先验$P$是$[0,1]^{K}$的简单分布。

​		2.假设动作之后的奖励只能取有限个不同的值，且先验$P$有有限的支持，记作$F$。然后我们可以关注汤普森抽样的基本概念和论证，不必关注积分和概率密度的复杂性。同时，下面的定义和引理适用于任意的先验和奖励分布。

​		3.最优臂$a$对于支持$P$的每个平均奖励向量$\mu$都是唯一的。

**3.1贝叶斯赌博机的更新**

​		贝叶斯统计的一个基本操作是贝叶斯更新：更新给定新数据的先验分布。

**3.1.1术语和符号**

​		算法在第$t$轮后采集的数据为动作-奖励对序列
$$
H_{t}=\left(\left(a_{1}, r_{1}\right), \dots,\left(a_{t}, r_{t}\right)\right) \in( A \times R )^{t}
$$
称为$t$-历史。$H_t$是一个随机变量，取决于平均奖励向量$\mu$、算法和奖励分布。

​		给定一个固定的序列
$$
H=\left(\left(a_{1}^{\prime}, r_{1}^{\prime}\right), \ldots,\left(a_{t}^{\prime}, r_{t}^{\prime}\right)\right) \in( A \times R )^{t}
$$
​		如果满足$Pr [H_t = H] > 0$，那么$H$称为可行$t$-历史，对应的赌博机算法称为$H$-一致算法。

​		$H$-诱导算法，它在每轮$s \in[t]$中都确定地选择臂 $a^{'}_{s}$。

​		设$H_t$为所有可行$t$-历史的集合。

​		因为每个奖励只能取有限多的值，所以$H_t$是有限的。

​		特别地，对于伯努利奖励分布和对于所有的臂$a$使得$Pr[\mu(a)\in(0,1)] = 1$的先验分布$P$，$H_t = (A \times\{0,1\})^{t}$ 。

​		然后，确定一个可行$t$-历史$H$，那么满足$H_t=H$条件下奖励概率为
$$
P _{H}( M ):=\operatorname{Pr_{\mu \sim P}}\left[\mu \in M | H_{t}=H\right], \quad \forall M \subset[0,1]^{K}
$$
​		对于任何$H$一致的赌博机算法，$P_H$是在$[0,1]^{K}$的奖励分布。

​		备注3.1：将经过$t$轮后的$P_H$称为贝叶斯后验概率。推导$P_H$的过程称为给定$H$条件下$P$的贝叶斯更新。

**3.1.2后验不依赖于算法**

​		贝叶斯赌博机的$P_H$既不依赖于$H$一致算法，也不依赖于$H$诱导算法。

​		定理3.1：所有$H$一致算法的分布$P_H$是相同的。

​		注意：算法的动作概率是由历史决定，奖励分布是由动作决定，奖励分布与历史无关。

​		推论3.2：重排列$H$得到$H'$，$P_H=P_H'$。

**3.1.3后验作为新的先验**

+ 引理3.3：如果$H'$是可行$t'$历史，那么$P _{H \oplus H^{\prime}}=\left( P _{H}\right)_{H^{\prime}}$。即
  $$
  P _{H \oplus H^{\prime}}( M )=\operatorname{Pr}_{\mu \sim P _{H}}\left[\mu \in M | H_{t^{\prime}}=H^{\prime}\right], \quad \forall M \subset[0,1]^{K}
  $$

+ 可行$t$历史：满足$Pr [H_t = H] > 0$的$H$

+ $t$历史：$1-t$轮采集的动作奖励序列为$H_t$

+ 可行$t$历史$H$和可行$t'$历史$H'$串联连接为$H\oplus H'$

+ 在$H_t=H$条件下的奖励概率分布为$P_H$

**3.1.4独立先验**

+ 引理3.4：如果先验$P$独立，那么$P _{H}\left(\cap_{a \in A } M _{a}\right)=\prod_{a \in A } P _{H}^{a}\left( M _{a}\right)$.