import numpy as np


def sigma(x):
    '''
    激活函数sigma(x)
    '''
    return 1 / (1 + np.exp(-x))


def sigma_diff(x):
    '''
    激活函数sigma(x)的导函数
    '''
    return sigma(x) * sigma(1 - x)


def softmax(x):
    '''
    激活函数softmax(x)
    '''
    return np.exp(x) / (np.vdot(np.ones(C), np.exp(x)))


# 类别数量
C = 10
# 样本维数
D = 3
# 训练集样本数量
N = 1

# 第0层神经元个数(特征向量的维数)
M_0 = D
# 第1层神经元个数
M_1 = 4
# 第2层神经元个数(类别数量)
M_2 = C

# 训练集特征矩阵
x_train = np.random.rand(N, D)

# 训练集标签
y_train = np.array([3, 0, 5, 1])
# one-hot向量
y_train_v = np.zeros((N, C))
for n in range(N):
    y_train_v[n, y_train[n]] = 1


# 第0层到第1层的权重矩阵
W_1 = np.random.rand(M_1, M_0)
# 第0层到第1层的偏置
b_1 = np.ones((M_1, 1))

# 第1层到第2层的权重矩阵
W_2 = np.random.rand(M_2, M_1)
# 第1层到第2层的偏置
b_2 = np.ones((M_2, 1))

sum = 0
for n in range(N):
    # 第0层神经元的输出(即x)
    a_0 = x_train[n, np.newaxis].T

    # 第1层神经元的净输入
    z_1 = np.dot(W_1, a_0) + b_1

    # 第1层神经元的输出
    a_1 = sigma(z_1)

    # 第2层神经元的净输入
    z_2 = np.dot(W_2, a_1) + b_2
    # 第2层神经元的输出(即y)
    a_2 = softmax(z_2)

    # 每个类的条件概率
    y_predict_v = a_2

    # 预测和真实的交叉熵损失
    loss = -np.vdot(y_train_v[n], np.log(y_predict_v))
    sum += loss

    # 第2层神经元的误差项
    delta_2 = y_train_v[n][:, np.newaxis] - y_predict_v
    # 第1层神经元的误差项
    delta_1 = sigma_diff(z_1) * (np.dot(W_2.T, delta_2))

    # loss关于第2层权重的梯度
    loss_diff_W_2 = np.dot(delta_2, a_1.T)
    print(loss_diff_W_2)
    # loss关于第2层偏置的梯度
    loss_diff_b_2 = delta_2
    print(loss_diff_b_2)

    # loss关于第1层权重的梯度
    loss_diff_W_1 = np.dot(delta_1, a_0.T)
    print(loss_diff_W_1)
    # loss关于第1层偏置的梯度
    loss_diff_b_1 = delta_1
    print(loss_diff_b_1)
'''
[[-0.13536116 -0.12394457 -0.12379283 -0.12716933]
 [-0.07018625 -0.06426662 -0.06418794 -0.06593869]
 [-0.1115168  -0.10211128 -0.10198627 -0.10476799]
 [ 0.76607058  0.70145891  0.70060013  0.71970929]
 [-0.15325486 -0.14032909 -0.14015729 -0.14398013]
 [-0.07138307 -0.0653625  -0.06528248 -0.06706308]
 [-0.06019597 -0.05511895 -0.05505146 -0.05655302]
 [-0.06081074 -0.05568187 -0.05561369 -0.05713058]
 [-0.06230384 -0.05704903 -0.05697918 -0.05853331]
 [-0.0410579  -0.03759501 -0.03754898 -0.03857314]]
[[-0.15414094]
 [-0.07992377]
 [-0.12698845]
 [ 0.87235393]
 [-0.17451718]
 [-0.08128663]
 [-0.06854746]
 [-0.06924753]
 [-0.07094776]
 [-0.0467542 ]]
[[ 0.01316122  0.04240845  0.0142904 ]
 [ 0.00895321  0.02884927  0.00972135]
 [ 0.02267798  0.07307362  0.02462366]
 [-0.03044363 -0.0980963  -0.03305556]]
[[ 0.04927337]
 [ 0.03351927]
 [ 0.08490249]
 [-0.11397575]]
'''