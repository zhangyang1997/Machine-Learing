**逻辑斯谛回归和最大熵模型**

+ 对数线性模型：包括逻辑斯谛回归模型和最大熵模型。
+ 学习算法：改进的迭代尺度算法和拟牛顿法。

**逻辑斯谛回归模型**

**1.逻辑斯谛分布**

+ 逻辑斯谛分布：连续随机变量$X$服从逻辑斯谛分布，$X$的分布函数和密度函数为

$$
\begin{aligned}
&F(x)=P(X \leqslant x)=\frac{1}{1+e^{-(x-\mu) / \tau}}\\
&f(x)=F^{\prime}(x)=\frac{ e ^{-(x-\mu) / y}}{\gamma\left(1+ e ^{-(x-\mu) / \gamma}\right)^{2}}
\end{aligned}
$$

+ 位置参数为$\mu$
+ 形状参数为$\gamma >0$

+ 逻辑斯谛分布函数是以点$(\mu,\frac{1}{2})$为中心对称S形曲线。

**2.二项逻辑斯谛回归模型**

+ 二项逻辑斯谛回归模型：分类模型，由条件概率$P(Y|X)$表示，$X$取值为实数，$Y$为$1$或$0$。
  $$
  \begin{aligned}
  &P(Y=1 | x)=\frac{\exp (w \cdot x+b)}{1+\exp (w \cdot x+b)}\\
  &P(Y=0 | x)=\frac{1}{1+\exp (w \cdot x+b)}
  \end{aligned}
  $$

+ 

+ 输出变量为$Y\in \{0,1\}$
+ 参数为$w\in \R^n$和$b\in \R$

**3.模型参数估计**
$$
设P(Y=1 | x)=\pi(x), \quad P(Y=0 | x)=1-\pi(x)\\
似然函数:\\\prod_{i=1}^{N}\left[\pi\left(x_{i}\right)\right]^{n}\left[1-\pi\left(x_{i}\right)\right]^{1-n}\\
对数似然函数:\\\begin{aligned}
L(w) &=\sum_{i=1}^{N}\left[y_{i} \log \pi\left(x_{i}\right)+\left(1-y_{i}\right) \log \left(1-\pi\left(x_{i}\right)\right)\right] \\
&=\sum_{i=1}^{N}\left[y_{i} \log \frac{\pi\left(x_{i}\right)}{1-\pi\left(x_{i}\right)}+\log \left(1-\pi\left(x_{i}\right)\right)\right] \\
&=\sum_{i=1}^{N}\left[y_{i}\left(w \cdot x_{i}\right)-\log \left(1+\exp \left(w \cdot x_{i}\right)\right]\right.
\end{aligned}\\
\hat{w}=argmax_{w} L(w)
$$

+ 训练集为$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$

**4.多项逻辑斯谛回归**

+ 离散型随机变量$Y$取值集合为$\{1,2,...,K\}$
  $$
  \begin{array}{c}
  P(Y=k | x)=\frac{\exp \left(w_{k} \cdot x\right)}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}, \quad k=1,2, \cdots, K-1 \\
  P(Y=K | x)=\frac{1}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}
  \end{array}
  $$

**最大熵模型**

**1.最大熵原理**

+ 最大熵原理：学习概率模型时，在所有可能的概率模型中，熵最大的模型时最好的模型。也表述为满足约束条件的模型集合中选取熵最大的模型。
+ 离散随机变量为$X$
+ 概率分布为$P(X)$
+ 熵为$H(P)=-\sum_{x} P(x) \log P(x)$且满足$0 \leqslant H(P) \leqslant \log |X|$
+ 随机变量$X$的取值个数为$|X|$
+ 当且仅当$X$的分布是均匀分布时熵最大$H(P)=\log|X|$

**2.最大熵模型**

+ 最大熵模型：假设满足所有约束条件的模型集合为
  $$
  C \equiv\left\{P \in \mathcal P | E_{P}\left(f_{i}\right)=E_{\tilde{p}}\left(f_{i}\right), \quad i=1,2, \cdots, n\right\}
  $$
  定义在条件概率分布$P(Y|X)$上的条件熵为$H(P)=-\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)$，则模型集合$C$中条件熵最大的模型成为最大熵模型。

+ 联合概率分布的经验分布为$\tilde{P}(X=x, Y=y)=\frac{v(X=x, Y=y)}{N}$
+ 训练集中样本$(x,y)$出现的频数为$v(X=x, Y=y)$
+ 边缘分布的经验分布为$\tilde{P}(X=x)=\frac{v(X=x)}{N}$
+ 训练集中$x$出现的频数为$v(X=x)$
+ 训练集容量为$N$
+ 特征函数为$f(x, y)=\left\{\begin{array}{ll}
  1, & \text{x与y满足某一事实} \\
  0, & \text {否则} 
  \end{array}\right.$
+ 特征函数关于经验分布的期望为$E_{\tilde{p}}(f)=\sum_{x, y} \tilde{P}(x, y) f(x, y)$
+ 特征函数关于模型$P(Y|X)$与经验分布$\tilde{P}(X)$的期望为$E_{P}(f)=\sum_{x, y} \tilde{P}(x) P(y | x) f(x, y)$
+ 模型学习的约束条件为$E_{p}(f)=E_{\tilde{p}}(f)$

**3.最大熵模型的学习**

+ 最大熵模型的学习等价于约束最优化问题：
  $$
  \begin{aligned}
  &\max _{\operatorname{Re} C} H(P)=-\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)\\
  &\begin{array}{ll}
  \text { s.t. } & E_{p}\left(f_{i}\right)=E_{\tilde{p}}\left(f_{i}\right), \quad i=1,2, \cdots, n \\
  & \sum_{y} P(y | x)=1
  \end{array}
  \end{aligned}
  $$

**4.最大熵模型学习中对偶函数极大化等价最大熵模型的极大似然估计**

