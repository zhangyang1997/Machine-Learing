**决策树**

+ 决策树：if-then规则的集合，定义在特征空间与类空间上的条件概率分布。
+ 决策树学习步骤：特征选择，决策树生成，决策树修剪。

**1.决策树模型与学习**

**1.1决策树模型**

+ 分类决策树模型：对实例进行分类的树形决策树。
+ 内部结点：特征或者属性。
+ 叶结点：类别。

**1.2决策树与if-then规则**

+ 决策树：if-then规则的集合。
+ 根结点到叶结点的每一条路径对应一条规则。
+ 路径上内部结点的特征对应规则的条件。
+ 路径上叶结点的类别对应规则的结论。

**1.3决策树与条件概率分布**

+ 决策树：给定特征条件下类别的条件概率分布。
+ 条件概率分布对应特征空间的划分，特征空间被划分为互不相交的单元。
+ 决策树的一条路径对应划分中的一个单元。
+ 特征的随机变量为$X$
+ 类的随机变量为$Y$
+ 条件概率分布为$P(Y|X)$

**1.4决策树学习**

+ 训练集$D=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$
+ 输入实例的特征向量$x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{ T }$
+ 特征数量为$n$
+ 类标签为$y_i \in {1,2,...K},i=1,2,...,N$
+ 训练集大小为$N$
+ 决策树学习：从训练数据集中归纳出一组分类规则/根据训练集估计条件概率模型。
+ 损失函数：正则化的极大似然函数。
+ 学习策略：最小化损失函数。
+ P问题：可以在多项式时间内求解的问题。
+ NP问题：可以在多项式时间内验证解是否正确的问题。
+ NP完全问题：所有的NP问题都可以多项式归约到NP完全问题。
+ 从所有可能的决策树中选择最优决策树是NP完全问题。
+ 选择最优树的方法：启发式方法近似求解。
+ 避免过拟合的方法：决策树剪枝，使决策树更简单，提高泛化能力。
+ 决策树的深度决定条件概率模型的复杂度。
+ 决策树的生成对应模型的局部选择。
+ 决策树的剪枝对应模型的全局选择。

**2.特征选择**

**2.1特征选择问题**

+ 特征选择：选择对训练数据具有分类能力的特征，信息增益大的特征分类能力强。
+ 特征选择的准则：信息增益或者信息增益比。

**2.2信息增益**

+ 熵：随机变量不确定性的度量。熵越大，随机变量的不确定性越大。

+ 随机变量$X$的概率分布为$P(X=x_i)=p_i,i=1,2,...,n$

+ 随机变量$X$的熵为$H(X)=-\sum_{i=1}^{n} p_{i} \log p_{i}$

+ 熵只与$X$的分布有关，与$X$的取值无关，熵可记作$H(p)=-\sum_{i=1}^{n} p_{i} \log p_{i}$

+ 随机变量$X,Y$的联合概率分布为$P\left(X=x_{i}, Y=y_{j}\right)=p_{i j}, \quad i=1,2, \cdots, n ; \quad j=1,2, \cdots, m$

+ 随机变量$X$给定的条件下随机变量$Y$的条件熵为$H(Y | X)=\sum_{i=1}^{n} p_{i} H\left(Y | X=x_{i}\right)$，其中$p_{i}=P\left(X=x_{i}\right), \quad i=1,2, \cdots, n$。

+ 经验熵和条件经验熵：通过数据估计得到的熵和条件熵。

+ **信息增益：得知特征$X$的信息而使得$Y$的信息不确定性减少的程度。特征$A$对训练集$D$的信息增益为$g(D,A)=H(D)-H(D|A)$**

+ 训练集$D$的经验熵为$H(D)$，表示对数据集$D$进行分类的不确定性。

+ 在特征$A$条件下训练集$D$的经验条件熵为$H(D|A)$，表示条件下对数据集$D$进行分类的不确定性

+ 互信息：熵$H(Y)$与条件熵$H(Y|X)$的差。

+ 决策树学习中的信息增益$g$等价于训练数据集中类和特征的互信息。

+ 基于信息增益的特征选择：给定训练集，计算每个特征的信息增益，选择信息增益最大的特征。

  + 符号说明：
    + 训练集为$D$
    + 训练集大小为$|D|$
    + 第$k$个类别为$C_k,k=1,2,...,K$
    + 类别数量为$K$
    + 训练集中属于类别$C_k$的样本数量为$C_k$
    + 特征为$A$
    + 特征$A$的$n$个不同的取值为$\{a_1,a_2,...,a_n\}$
    + 根据特征$A$不同取值将训练集$D$划分为$\{D_1,D_2,...,D_n\}$
    + 第$i$个子集$D_i$ 中样本数量为$|D_i|$
    + 子集$D_i$中属于类别$C_k$样本的集合为$D_{ik}$，集合大小为$|D_{ik}|$

  + ①计算训练集$D$的经验熵$H(D)$
    $$
    H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}
    $$

  + ②计算每个特征$A$对数据集$D$的经验条件熵
    $$
    H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{ik}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{ik}\right|}{\left|D_{i}\right|}
    $$

  + ③计算每个特征$A$对数据集$D$的信息增益
    $$
    g(D, A)=H(D)-H(D | A)
    $$

+ 信息增益比$g_{R}(D, A)=\frac{g(D, A)}{H(D)}$
