**朴素贝叶斯**

+ 朴素贝叶斯：基于贝叶斯定理与特征条件独立假设的分类方法。
+ 朴素：特征条件独立假设。

**1.朴素贝叶斯法的学习与分类**

**1.1基本方法**

+ 输入空间为$\mathcal{X}\sube R^{n}$

+ 输出空间为$\mathcal{Y}=\{c_1,c_2,...,c_K\}$

+ 输入特征向量为$x\in \mathcal{Y}$

+ 输出类标记为$y\in \mathcal{Y}$

+ 输入空间上的随机向量为$X$

+ 输出空间上的随机变量为$Y$

+ $X$和$Y$的联合概率分布为$P(X,Y)$

+ 训练数据集为$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$

+ 训练数据集独立同分布服从联合概率分布$P(X,Y)$

+ 先验概率分布为$P\left(Y=c_{k}\right), \quad k=1,2, \cdots, K$

+ 条件概率分布为
  $$
  P\left(X=x | Y=c_{k}\right)=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right), \quad k=1,2, \cdots, K
  $$

+ 输入特征向量$x$的第$i$维特征的值为$x^{(i)}$

+ $x^{(i)}$可取值数量为$S_i$

+ $Y$可取值数量为$K$

+ 条件概率分布的参数数量为$K\prod_\limits{i=1}^{n}S_i$

+ 特征条件独立性假设为
  $$
  \begin{aligned}
  P\left(X=x | Y=c_{k}\right) &=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right) \\
  &=\prod_{j=1}^{n} P\left(X^{(n)}=x^{(n)} | Y=c_{k}\right)
  \end{aligned}
  $$

+ 后验概率为$P\left(Y=c_{k} | X=x\right)$

+ 后验概率计算公式为
  $$
  P\left(Y=c_{k} | X=x\right)=\frac{P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}{\sum_{k} P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}\\
  =\frac{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}, \quad k=1,2, \cdots, K
  $$

+ 朴素贝叶斯分类器为
  $$
  y=f(x)=\arg \max _{c_{k}} \frac{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}\\
  因为分母对所有的c_k相同,所以\\
  y=\arg \max _\limits{c_k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(1)}=x^{(j)} | Y=c_{k}\right)
  $$

**1.2后验概率最大化等价于期望风险最小化**

+ 后验概率最大化为$f(x)=\arg \max \limits_{c_k} P\left(c_{k} | X=x\right)$

+ 期望风险最小化为$f(x)=\arg \min \limits_{y \in \mathcal{Y}} \sum_{k=1}^{K} L\left(c_{k}, y\right) P\left(c_{k} | X=x\right)$

+ 后验概率为$P\left(Y=c_{k} | X=x\right)$

+ 期望风险为$R_{\operatorname{exp}}(f)=E[L(Y, f(X))]=E_{x} [\sum_{k=1}^{K}\left[L\left(c_{k}, f(X)\right)\right] P\left(c_{k} | X\right)]$

+ 0-1损失函数：$L(Y, f(X))=\left\{\begin{array}{ll}
  1, & Y \neq f(X) \\
  0, & Y=f(X)
  \end{array}\right.$

+ 分类决策函数为$f(X)$

+ 联合概率分布为$P(X,Y)$

+ 证明
  $$
  \begin{aligned}
  期望风险最小化&=
  \arg \min _{y \in \mathcal{Y} } \sum_{k=1}^{K} L\left(c_{k}, y\right) P\left(c_{k} | X=x\right) \\
  &=\arg \min _{y \in \mathcal{Y} } \sum_{k=1}^{K} P\left(y \neq c_{k} | X=x\right) \\
  &=\arg \min _{y \in \mathcal{Y} }\left(1-P\left(y=c_{k} | X=x\right)\right) \\
  &=\arg \max _{y \in \mathcal{Y} } P\left(y=c_{k} | X=x\right)\\
  &=\arg \max \limits_{c_k} P\left(c_{k} | X=x\right)\\
  &=后验概率最大化
  \end{aligned}
  $$

**2.朴素贝叶斯法的参数估计**

**2.1极大似然估计**

+ 朴素贝叶斯学习：估计先验概率$P(Y=c_k)$和条件概率$P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)$

+ 先验概率的极大似然估计：$P\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K$

+ 特征向量$x$的第$j$个特征为$x^{(j)}$

+ $x^{(j)}$可能取值的集合为$\left\{a_{j_{1}}, a_{j_{2}}, \cdots, a_{j s}\right\}$

+ 条件概率的极大似然估计为
  $$
  \begin{aligned}
  &P\left(X^{(j)}=a_{j l} | Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}\\
  &j=1,2, \cdots, n ; \quad l=1,2, \cdots, S_{j} ; \quad k=1,2, \cdots, K
  \end{aligned}
  $$

+ 第$i$个样本的第$j$个特征为$x_i^{(j)}$

+ 第$j$个特征可能取的第$l$个值为$a_{jl}$

+ 指示函数为$I$

**2.2学习与分类算法**

+ 朴素贝叶斯算法

  ```
  输入:
  训练集T={(x[1],y[1]),(x[2],y[2]),...,(x[N],y[N])}和实例x
  其中:
  x[i]=(x[i][1],x[i],[2],...,x[i][n])
  x[i][j]是第i个样本的第j个特征
  x[i][j]属于{a[j][1],a[j][2],...,a[j][S[j]]}
  a[i][l]是第j个特征可能取的第l个值
  j属于[1,2,...,n]
  l属于[1,2,...,S[j]]
  y[i]属于{c[1],c[2],...,c[K]}
  
  输出:
  实例x的分类
  
  1.计算先验概率p1[k]和条件概率p2[j][l][k]
  2.对于给定的实例x，计算后验概率p[k]=p1[k]*prod(p2[j][l][k],n)
  3.确定实例x的类，y=argmax(p)
  ```


**2.3贝叶斯估计**

+ 条件概率的贝叶斯估计为
  $$
  P_{\lambda}\left(X^{(j)}=a_{j l} | Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)+\lambda}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)+S_{j} \lambda}
  $$

  + 当$\lambda=1$，对应拉普拉斯平滑
  + 当$\lambda=0$，对应极大似然估计
  + 贝叶斯估计的作用：通过$\lambda$使估计的条件概率不为0。

+ 先验概率的贝叶斯估计为
  $$
  P_{\lambda}\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)+\lambda}{N+K \lambda}
  $$
  