**一.线性SVM原始问题**
$$
\begin{aligned}
&\min _{w, b, \xi} \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}\\
&\text { s.t. } \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N\\
&\xi_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{aligned}
$$
$\xi_i$是每个样本的松弛变量

$C$是惩罚参数，$C$越大误分类惩罚越大。

分类超平面和决策函数为
$$
\begin{aligned}
&w^{*} \cdot x+b^{*}=0\\
&f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)
\end{aligned}
$$
**二.线性SVM对偶问题的学习算法**

输入训练集，输出分类超平面和分类决策函数。

1.选择惩罚参数$C>0$，构造并凸二次规划问题
$$
\begin{array}{ll}
\min _{\alpha} & \frac{1}{2} \sum_\limits{i=1}^{N} \sum_\limits{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_\limits{i=1}^{N} \alpha_{i} \\
\text { s.t. } & \sum_\limits{i=1}^{N} \alpha_{i} y_{i}=0 \\
& 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{array}
$$
2.计算$w^*$和$b^*$(选择一个$0<\alpha_j<C$对应的$j$)
$$
w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}\\
b^{*}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{*}\left(x_{i} \cdot x_{j}\right)
$$
3.分离超平面和分类决策函数
$$
\begin{aligned}
&w^{*} \cdot x+b^{*}=0\\
&f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)
\end{aligned}
$$
**三.线性SVM等价问题**
$$
\min _{w, b} \sum_{i=1}^{N}\left[1-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2}
$$

$$
[z]_{+}=\left\{\begin{array}{ll}
z, & z>0 \\
0, & z \leqslant 0
\end{array}\right.
$$

