**一.简单推导**

1.超平面$(w,b)$关于样本点的函数间隔为$\hat{\gamma}_{i}=y_{i}\left(w \cdot x_{i}+b\right)$

2.超平面$(w,b)$关于训练集的函数间隔为$\hat{\gamma}=\min\limits_{i=1,-N} \hat{\gamma}_{i}$

3.超平面$(w,b)$关于样本点的几何间隔为$\gamma_{i}=y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right)$

4.超平面$(w,b)$关于训练集的几何间隔为$\gamma=\min\limits_{i=1,-\infty} \gamma_{i}$

5.函数间隔和几何间隔的关系$\gamma_{i}=\frac{\hat{\gamma}_{i}}{\| w \|},\gamma=\frac{\hat{\gamma}}{\| w \|}$

6.最大间隔分离超平面指最大化超平面关于训练集的几何间隔，下面为约束优化问题
$$
\begin{aligned}
&\max _{x, b} \gamma\\
&\text { s.t. } \quad y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right) \geqslant \gamma, \quad i=1,2, \cdots, N
\end{aligned}
$$
7.几何间隔用函数间隔代替，等价于下面优化问题
$$
\begin{aligned}
&\max _{w, b} \frac{\hat{\gamma}}{\|w\|}\\
&\text { s.t. } \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant \hat{\gamma}, \quad i=1,2, \cdots, N
\end{aligned}
$$
8.函数间隔取值不影响最优化的解，等价于下面优化问题
$$
\begin{aligned}
&\min _{x, b} \frac{1}{2}\|w\|^{2}\\
&y_{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\end{aligned}
$$
**二.线性可分支持向量机的学习算法—最大间隔法**

输入：线性可分训练集$T=\{(x_1,y_1),...,(x_N,y_N)\}$，$x_i$是$n$维向量，$y_i\in\{-1,+1\}$是标量。

输出：最大间隔分离超平面和分类决策函数。

1.构造并求解约束最优化问题：
$$
\begin{array}{ll}
\min _\limits{w, b} & \frac{1}{2}\|w\|^{2} \\
\text { s.t. } & y_{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$
求得最优解$w^*,b^*$.

2.分离超平面和分类决策函数：
$$
\begin{aligned}
&w^{*} \cdot x+b^{*}=0\\
&f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)
\end{aligned}
$$
**三.如何求解上面最优化问题？**

1.对原始问题使用拉格朗日乘数法，然后转化为原始问题的对偶问题。
$$
\begin{aligned}
\min _{\alpha} & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } & \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
& \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{aligned}
$$
求得最优解$\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{N}^{*}\right)^{T}$

2.计算$w^*$和$b^*$(任选一个$\alpha_j^*>0$对应的$j$，计算$b^*$)
$$
w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}\\
b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right)\\
$$
3.分离超平面和分类决策函数
$$
\begin{aligned}
&w^{*} \cdot x+b^{*}=0\\
&f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)
\end{aligned}
$$
**四.线性可分SVM举例**

训练集$x_1=(3,3)^{T},x_2=(4,3)^{T},x_3=(1,1)^{T}$，$y_1=1,y_2=1,y_3=-1$。

解：
$$
\begin{aligned}
&\begin{aligned}
\min _{a} & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
&=\frac{1}{2}\left(18 \alpha_{1}^{2}+25 \alpha_{2}^{2}+2 \alpha_{3}^{2}+42 \alpha_{1} \alpha_{2}-12 \alpha_{1} \alpha_{3}-14 \alpha_{2} \alpha_{3}\right)-\alpha_{1}-\alpha_{2}-\alpha_{3}
\end{aligned}\\
&\text { s.t. } \quad \alpha_{1}+\alpha_{2}-\alpha_{3}=0\\
&\alpha_{i} \geqslant 0, \quad i=1,2,3\\
&s\left(\alpha_{1}, \alpha_{2}\right)=4 \alpha_{1}^{2}+\frac{13}{2} \alpha_{2}^{2}+10 \alpha_{1} \alpha_{2}-2 \alpha_{1}-2 \alpha_{2}\\
&求s偏导令其为0，s在(\frac{3}{2},-1)^{T}处取极值，但是不满足约束条件\alpha\geq0,所以最小值应该在边界上。\\
&\alpha_1=0,s\left(0, \frac{2}{13}\right)=-\frac{2}{13}\\
&\alpha_1=0,s\left(\frac{1}{4},0\right)=-\frac{1}{4}\\
&所以s_{min}=s\left(\frac{1}{4},0\right)=-\frac{1}{4}\\
&\alpha_{1}=\frac{1}{4},\alpha_{2}=0,\alpha_{3}=\alpha_{1}+\alpha_{2}=\frac{1}{4}\\
&\begin{aligned}
&w_{1}^{*}=w_{2}^{*}=\frac{1}{2}\\
&b^{*}=-2
\end{aligned}\\
&\begin{aligned}
&\frac{1}{2} x^{(1)}+\frac{1}{2} x^{(2)}-2=0\\
&f(x)=\operatorname{sign}\left(\frac{1}{2} x^{(1)}+\frac{1}{2} x^{(2)}-2\right)
\end{aligned}

\end{aligned}
$$
