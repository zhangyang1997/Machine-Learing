**$k$近邻法**

+ $k$近邻法：一种分类和回归方法，本书只讨论分类问题。没有学习过程，利用训练集对特征向量空间进行划分，作为其分类的模型。

**1.$k$近邻算法**

+ $k$近邻算法：给定一个训练集，对新的输入实例，在训练集中找到与该实例最邻近的$k$个实例，输入实例的类别是$k$个邻近实例中的多数类。

```pseudocode
输入:
训练集T={(x[1],y[1]),(x[2]y[2]),...,(x[N],y[N])}
x[i]是n维向量
y[i]属于{c[1],c[2],...,c[K]}

输出:
实例x_test所属的类y_test

1.计算x_test和所有x的距离,
得到dist_label={(dist[1],label[1]),..,(dist[N],label[N])}
2.按照距离sort
3.
count={<class[0],0>,...<class[K],,0>}
for i in range(k):
	c=distlabel[i][1];
	count[c]++
4.y_test=argmax(count) #分类决策规则(多数表决)
```

**2.$k$近邻模型**

+ $k$近邻模型的三个基本要素：距离度量，$k$值的选择，分类决策规则。

**2.1模型**

+ 单元：在特征空间中，对每个训练实例点$x_i$，距离该点比其他点更近的所有点组成一个区域。

**2.2距离度量**

+ 特征空间：$n$维实数向量空间$R^{n}$。
+ 闵式距离：曼哈顿距离，欧式距离，切比雪夫距离。

**2.3$k$值的选择**

+ $k$越小，模型越复杂，越容易过拟合。

+ $k$越大，模型越简单，越不容易过拟合。

**2.4分类决策规则**

+ 多数表决规则：由输入实例的$k$个邻近的训练实例中的多数类决定输入实例的类。
+ 对数表决规则等价于经验风险最小化。

**3.$k$近邻法的实现:$kd$树**

**3.1构造$kd$树**

+ $k$维数据的二叉平衡搜索树。

```pseudocode
输入:
T={x[0],x[1],...x[N-1]}
x[i]={x[i][0],x[i][1],...,x[i][k-1]}

输出
kdtree

#1.计算每一维方差,split = 最大方差对应的维数序号 = argmax(x);
1.split = 0
2.根据split维对x[:,split]数据进行排序，计算中位数m;
3.node-data = 中位数m所在的样本点;
4.根据m递归分割左子空间和右子空间;
  split = (j + 1) % k;#(j+1)是树的深度.
```

**3.2搜索$kd$树**

```pseudocode
输入:
kdtree
target

输出:
target的k近邻

1.在kdtree中查找包含target的叶子节点:从root结点出发，递归向下访问kdtree;
	如果target[node.split]<node.data[node.split],node=node.left;
	如果target[node.split]>node.data[node.split],node=node.right;
	直到node.left=null且node.right=null.
	将访问的每个<node,-dist>加入最小堆h,堆顶是超球体最大半径对应的node。
3.递归向上，对于每个结点node:如果以target为球心的最大超球体半径大于target到切分面距离,遍历node的另一侧子结点；否则继续回退。
4.当回退到根结点时，搜索结束，最小堆存储了k个近邻点。
```

