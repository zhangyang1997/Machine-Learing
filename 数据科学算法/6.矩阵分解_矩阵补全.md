**因为SVD分解只能对没有缺失值的矩阵进行分解，所以对评分矩阵$R$进行补全。使用的方法是矩阵分解和梯度下降法实现矩阵补全。**


$$
\begin{aligned}
\min _{ P ^{*}, Q ^{*}} J ( R ; P , Q ) &=\frac{1}{2} \sum_{(i, j) \in \mathcal{R} }\left(r_{i j}-\hat{r}_{i j}\right)^{2} \\
&=\frac{1}{2} \sum_{(i, j) \in \mathcal{R} }\left(r_{i j}- p _{i}^{T} q _{j}\right)^{2}
\end{aligned}
$$
符号说明：

+ $P^*$为分解后的最优$P$矩阵
+ $Q^*$为分解后的最优$Q$矩阵

+ $J$为平方损失函数
+ $R$为评分矩阵($m\times n$)
+ $P$为分解后的$P$矩阵($r\times m$)
+ $P^T$为$P$的转置$(m\times r)$
+ $Q$为分解后的$Q$矩阵($r\times n$)
+ $\mathcal{R}$为评分矩阵中能观测到评分的用户$i$，项目$j$位置的评分组成的集合$\mathcal{R}=\{(i,j)|r_{ij}\neq0\}$

+ $r_{ij}$为真实评分
+ $\hat{r}_{ij}$为预测评分
+ $p_i^T$为$P^T$矩阵的第$i$行
+ $q_j$为$Q$矩阵的第$j$列

**梯度下降法**
$$
e_{i j}^{2}=\left(r_{i j}-\hat{r}_{i j}\right)^{2}=\left(r_{i j}-\sum_{k=1}^{K} p_{i k} q_{j k}\right)^{2}\\
\min _{P^{*}, Q^{*}} J ( R ; P , Q )=\frac{1}{2} \sum_{(i, j) \in R } e_{i j}^{2}\\
\begin{array}{l}
\frac{\partial}{\partial p_{i k}} e_{i j}^{2}=-e_{i j} q_{i k} \\
\frac{\partial}{\partial q_{j k}} e_{i j}^{2}=-e_{i j} q_{j k}
\end{array}\\
$$

$$
\begin{array}{l}
p_{i k}^{(t+1)} \leftarrow p_{i k}^{(t)}+\epsilon_{t+1} \sum_{j:(i, j) \in R } e_{i j}^{(t)} q_{j k}^{(t)} \\
q_{j k}^{(t+1)} \leftarrow q_{j k}^{(t)}+\epsilon_{t+1} \sum_{i:(i, j) \in R } e_{i j}^{(t)} p_{i k}^{(t)}
\end{array}\\
e_{i j}^{(t)}=r_{i j}-\sum_{k=1}^{K} p_{i k}^{(t)} q_{j k}^{(t)}\\
\begin{array}{l} 
P ^{(t+1)} \leftarrow(1-\lambda) P ^{(t)}+\epsilon_{t+1} E ^{(t)} Q ^{(t)} \\
Q ^{(t+1)} \leftarrow(1-\lambda) Q ^{(t)}+\epsilon_{t+1} E ^{(t)^{T}} P ^{(t)}
\end{array}
$$



