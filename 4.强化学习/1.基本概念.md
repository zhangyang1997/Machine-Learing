**1.基本概念**

+ 强化学习：机器将当前状态映射到动作使累计获得的奖励最大化。
+ 强化学习的马尔可夫决策过程描述：
  + 状态：机器对环境的感知，记作$x$。
  + 状态空间：所有可能的状态的集合，记作$X$。
  + 动作：机器所采取的动作，记作$a$。
  + 动作空间：所有能采取的动作的集合，记作$A$。
  + 转移概率：执行某个动作之后，当前状态以某种概率转移到另一个状态，概率记作$P$。
  + 奖赏函数：状态转移的同时，环境反馈给机器的奖赏，记作$r$。
  + 第$t$步获得的奖赏值：记作$r_t$。
+ 确定性策略：$\pi(x)=a$，在状态$x$下执行动作$a$。
+ 随机性策略：$P=\pi(x,a)$，在状态$x$下执行动作$a$的概率。
+ 强化学习和监督学习的区别：强化学习的数据没有标签信息，数据之间不是独立同分布。
+ 强化学习和监督学习的联系：
  + 状态对应属性。
  + 动作对应标记。
  + 确定性策略对应回归器、决策函数、非概率模型。
  + 随机性策略对应分类器、条件概率分布、概率模型。
  + 强化学习是具有“延迟标签”的监督学习问题。
+ 长期累积奖赏计算：
  + $T$步累积奖赏：执行该策略$T$步的平均奖赏的期望值，公式为 $E \left[\frac{1}{T} \sum\limits_{t=1}^{T}r_{t}\right]$。
  + $\gamma$折扣累积奖赏：执行到最后，同时越往后的奖赏权重越低，公式为$E \left[\sum_\limits{t=0}^{+\infty} \gamma^{t} r_{t+1}\right]$。