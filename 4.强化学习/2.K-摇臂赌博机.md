**1.基本概念**

+ 单步强化学习：在状态$x$下执行一次动作$a$得到最大化单步奖赏。
  + ①已知每个动作的奖赏值，将每个动作尝试一边得到最大单步奖赏。
  + ②每个动作的奖赏值未知，但是服从一个概率分布，进行多次尝试估计每个动作的奖赏值。
+ K-摇臂赌博机：赌博机有K个摇臂，赌徒在投入一个硬币后可以选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币，赌徒不知道这个概率，赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。
  + K-摇臂赌博机对应单步强化学习的②，且尝试动作的次数有限。
+ 仅探索法：将尝试的机会平均分给每一个动作，即轮流执行动作，最终将每个动作的平均奖赏作为期望奖赏的近似值。
+ 探索动作：以均匀概率随机选取一个摇臂。
+ 仅利用法：将尝试的机会分给当前平均奖赏值最大的动作。
+ 利用动作：选择当前平均奖赏最高的摇臂(若有多个，则随机选取一个)。

**2.$\epsilon$-贪心算法**

+ $\epsilon$-贪心摇臂：每次尝试时，以$\epsilon$的概率进行探索动作；以$1-\epsilon$的概率进行利用动作。

+ 全量式计算摇臂$k$的平均奖赏

  + 原理：尝试$n$次后计算平均奖赏。

  + 符号说明：尝试次数为$n$，摇臂序号为$k$，第$i$次尝试的奖赏值为$v_i$，第$n$次尝试后摇臂$k$的平均奖赏为$Q(k)$。

  + 公式

    + $$
      \begin{align}
      Q(k)=\frac{1}{n} \sum\limits_{i=1}^{n} v_{i}
      \end{align}
      $$

+ 增量式计算摇臂$k$的平均奖赏。

  + 原理：每尝试一次就立即更新平均奖赏。

  + 符号说明：尝试次数为$n$，摇臂序号为$k$，第$n$次尝试的奖赏值为$v_n$，第$n$次尝试后的平均奖赏为$Q_n(k)$。

  + 公式

    + $$
      \begin{align}
      & Q_0(k)=0\\
      & Q_n(k)=\frac{1}{n}((n-1)\times Q_{n-1}(k)+v_n)
      =Q_{n-1}(k)+\frac{1}{n}(v_n-Q_{n-1}(k))
      \end{align}
      $$

+ 增量式对比全量式计算平均奖赏的优点
  
+ 全量式计算需要存储尝试次数$n$和对应的$n$个奖赏值，占用空间为$n+1$；增量式计算需要存储第$n-1$次尝试后的平均奖赏$Q_{n-1}$和第$n$次尝试的奖赏值$v_n$和已尝试次数$n-1$，占用空间为$3$。
  
+ $\epsilon$-贪心算法伪代码

  ```pseudocode
  输入:摇臂数量K,奖赏函数R,尝试次数T,探索概率e.
  过程:
  r = 0;  #累积奖赏
  for i = 1, 2, ..., K do
  	Q[i] = 0;  #摇臂i的平均奖赏
  	count[i] = 0;  #选择摇臂i的次数
  end for
  for t = 1, 2, ..., T do
  	if rand() < e then  #如果[0,1]随机数的小于探索概率，那么执行探索动作
  		k = 从1,2,...,K中以均匀分布随机选取;
  	else  #否则，执行利用动作
  		k = argmax(Q);  #返回最大平均奖赏的摇臂序号
  	end if
  	v = R(k);  #第t尝试时摇臂k中的奖赏为R(k)
  	r = r + v;  #累积奖赏
  	Q[k] =  (Q[k] * count[k] + v) / (count[k] + 1);  #更新第t次尝试后摇臂k的平均奖赏
  	count[k] = count[k] + 1;  #累计选择摇臂k的次数
  end for
  输出:累积奖赏r.
  ```

+ 如何选取$\epsilon$？

  + 如果reward分布越均匀，不确定性越大，那么需要更多的探索，此时需要较大的$\epsilon$。
  + 如果reward分布越集中，不确定性越小，那么需要较少的探索，此时需要较小的$\epsilon$。
  + 通常令$\epsilon$取一个较小的常数，如0.1或者0.01。
  + 因为随着尝试次数的增加，摇臂奖赏的概率分布可以近似得到，那么可以让$\epsilon$随着尝试次数的增加而逐渐减小，例如令$\epsilon=\frac{1}{\sqrt{t}}$。

**3.Softmax算法**

+ Softmax摇臂：每次尝试动作的概率由玻尔兹曼(Boltzmann)分布决定。

+ 玻尔兹曼分布

  + 符号说明：摇臂序号$k$，摇臂数量$K$，温度$\tau>0$，当前摇臂$k$的平均奖赏$Q(k)$。

  + 公式

    + $$
      P(k)=\frac{e^{\frac{Q(k)}{\tau}}}{\sum\limits_{i=1}^{K} e^{\frac{Q(i)}{\tau}}}
      $$

  + 说明：$\tau$越大，$P(k)$之间的差距越小，趋向探索动作；$\tau$越小，$P(k)$之间的差异越大，趋向利用动作。

+ Softmax算法伪代码

  ```pseudocode
  输入:摇臂数量K,奖赏函数R,尝试次数T,温度参数tau.
  过程:
  r = 0;
  for i = 1, 2, ..., K do
  	Q[i] = 0;  #摇臂i的平均奖赏
  	count[i] = 0;  #选择摇臂i的次数
  end for
  for t = 1, 2, ..., T do
  	k = 从1,2,...,K中根据玻尔兹曼分布随机选取。
  	v = R(k);
  	r = r + v;
  	Q[k] = (Q[k] * count[k] + v) / (count[k] + 1);
  	count[k] = count[k] + 1;
  end for
  输出:累积奖赏r.
  ```

+ 如何选取tau?
  
  + 需要更多探索，tau越大；需要更多利用，tau越小。

**4.总结**

+ 如何解决离散状态空间、离散动作空间是上的多步强化学习任务？
  + 直接办法：将每个状态上的动作的选择看作一个K-摇臂赌博机问题。
    + 用强化学习任务的累积奖赏代替K臂赌博机中的奖赏函数R。
  + 缺点：没有考虑强化学习任务马尔可夫决策过程的结构。